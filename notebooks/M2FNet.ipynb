{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pEtX0AFgXce4",
        "90rkyIzDcLJE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGujSRXBwNDd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from transformers import XLMRobertaModel, RobertaModel, RobertaTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture"
      ],
      "metadata": {
        "id": "sL3XUD2bXhWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderModule(nn.Module):\n",
        "    def __init__(self, n_layers=6):\n",
        "        super(EncoderModule).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.encoders = nn.ModuleList()\n",
        "        for _ in range(self.n_layers):\n",
        "            self.encoders.append(nn.TransformerEncoderLayer(d_model=64, nhead=8, batch_first=True)) # d_model - длина последовательности, подумать над тем какую длину использовать\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.n_layers):\n",
        "            x = x + self.encoders[i](x)\n",
        "        return x\n",
        "\n",
        "class FusionAttentionModule(nn.Module):\n",
        "    def __init__(self, n_layers=5):\n",
        "        self.n_layers = n_layers\n",
        "        self.audio_attn = nn.ModuleList()\n",
        "        self.video_attn = nn.ModuleList()\n",
        "        self.fcs = nn.ModuleList()\n",
        "        for _ in range(self.n_layers):\n",
        "            self.audio_attn.append(nn.MultiHeadAttention(768, 8, kdim=128))\n",
        "            self.video_attn.append(nn.MultiHeadAttention(768, 8, kdim=600))\n",
        "            self.fcs.append(nn.Linear(768 + 600 + 128, 768))\n",
        "\n",
        "    def forward(self, audio_emb, text_emb, video_emb):\n",
        "        text_state = text_emb\n",
        "        for i in range(self.n_layers):\n",
        "            audio_attn_out = self.audio_attn[i](text_state, audio_emb, text_state)\n",
        "            video_attn_out = self.video_attn[i](text_state, video_emb, text_state)\n",
        "            emb_cat = torch.cat([audio_attn_out, text_state, video_attn_out], dim=1)\n",
        "            text_state = self.fcs[i](emb_cat)\n",
        "        return text_state"
      ],
      "metadata": {
        "id": "-uE9dN9VqNhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Feature Extractor"
      ],
      "metadata": {
        "id": "Xs5iBvBG6HNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text feature extractor\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained('tae898/emoberta-base')\n",
        "model = AutoModel.from_pretrained('tae898/emoberta-base')\n",
        "\n",
        "enc = tokenizer([\"Привет как дела\"], return_tensors='pt')\n",
        "model(**enc).last_hidden_state.mean(dim=1).size() # last_hidden_state/pooler_output ?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7s1dGD-y5S0",
        "outputId": "a9e12897-2bfa-4674-c9e3-598a1c3e4419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at tae898/emoberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class M2FNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(M2FNet, self).__init__()\n",
        "\n",
        "        # self.audio_feature_extractor = AudioFeatureExtractor()\n",
        "        # self.text_feature_extractor = TextFeatureExtractor()\n",
        "        # self.video_feature_extractor = VideoFeatureExtractor()\n",
        "\n",
        "        self.audio_encoder = EncoderModule()\n",
        "        self.text_encoder = EncoderModule()\n",
        "        self.video_encoder = EncoderModule()\n",
        "\n",
        "        self.fusion_attn = FusionAttentionModule()\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=768 + 600 + 128, out_features=768 + 600 + 128) # дописать размерности\n",
        "        self.fc2 = nn.Linear(in_features=768 + 600 + 128, out_features=7) # дописать размерности\n",
        "\n",
        "    def forward(self, audio_emb, text_emb, video_emb):\n",
        "        \"\"\"\n",
        "        audio_emb - (B, UtrLen, Hid_a) ??? поправить когда будет известен формат данных\n",
        "        text_emb - (B, UtrLen, Hid_t) (B, UttLen, 768) поправить когда будет известен формат данных\n",
        "        video_emb - (B, UtrLen, Hid_v) (B, UttLen, 600) поправить когда будет известен формат данных\n",
        "        \"\"\"\n",
        "\n",
        "        audio_enc_out = self.audio_encoder(audio_emb) # (B, UtrLen, Hid_a)\n",
        "        text_enc_out = self.text_encoder(text_emb) # (B, UtrLen, Hid_t) (B, UttLen, 768)\n",
        "        video_enc_out = self.video_encoder(video_emb) # (B, UtrLen, Hid_v) (B, UttLen, 600)\n",
        "\n",
        "        fusion_out = self.fusion_attn(audio_enc_out, text_enc_out, video_enc_out) # (B, UtrLen, ?)\n",
        "\n",
        "        concat_out = torch.cat([audio_enc_out, fusion_out, video_enc_out], dim=-1)\n",
        "\n",
        "        fc1_out = self.fc1(concat_out)\n",
        "        fc2_out = self.fc2(fc1_out)\n",
        "\n",
        "        return fc2_out"
      ],
      "metadata": {
        "id": "8_LMrHxMxKdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual Feature Extractor"
      ],
      "metadata": {
        "id": "pEtX0AFgXce4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Получить эмбеддинг сцены (кадра)\n",
        "2. Взять максимум по 0 размерности (чтобы остался 1 вектор размерности hidden_dim)\n",
        "3. Детектировать лица на кадре\n",
        "4. Получить из кропнутых лиц эмбеддинги\n",
        "5. Посчитать взвешенную сумму на основе площадей боксов лиц\n",
        "6. Взять максимум по 0 размерности (чтобы остался 1 вектор размерности hidden_dim)\n",
        "7. Конкатенировать эмбеддинг сцены и эмбеддинг лиц\n",
        "\n",
        "*Фейс эмбеддеры лучше поискать готовые (по совету Савченко)*"
      ],
      "metadata": {
        "id": "JmBuW_adX7pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adaptive_margin_triplet_loss(anchor, positive, negative, margin=1.0):\n",
        "    \"\"\"\n",
        "    Функция для адаптивной триплетной потери с динамическим порогом\n",
        "    \"\"\"\n",
        "    # Вычисление евклидова расстояния между парами\n",
        "    dist_anchor_positive = nn.PairwiseDistance(anchor, positive, p=2)\n",
        "    dist_anchor_negative = nn.PairwiseDistance(anchor, negative, p=2)\n",
        "    dist_positive_negative = nn.PairwiseDistance(positive, negative, p=2)\n",
        "    # Адаптивное пороговое значение\n",
        "    adaptive_margin = 2 + (2 / torch.exp(4 * dist_anchor_positive)) + (2 / torch.exp(-4 * dist_anchor_negative + 4))\n",
        "\n",
        "    # Вычисление триплетной потери с адаптивным порогом\n",
        "    loss = dist_anchor_positive - (dist_anchor_negative + dist_positive_negative) / 2 + adaptive_margin\n",
        "    return loss\n",
        "\n",
        "def variance_loss(embeddings, epsilon=1e-6): # не забыть сложить по a, p, n\n",
        "    variance = embeddings.var(dim=0)\n",
        "    return torch.mean(1 - torch.sqrt(variance + epsilon))\n",
        "\n",
        "# def covariance_loss(embeddings):\n",
        "#     n = embeddings.size(0)\n",
        "#     cov_matrix = torch.mm((embeddings - embeddings.mean(dim=0)).T, embeddings - embeddings.mean(dim=0)) / (n - 1)\n",
        "#     cov_loss = torch.sum(cov_matrix ** 2) - torch.sum(torch.diag(cov_matrix) ** 2)\n",
        "#     return cov_loss / embeddings.size(1)\n",
        "\n",
        "def covariance_loss(embeddings):\n",
        "    covariance = embeddings.cov()\n",
        "    covariance.fill_diagonal_(0)\n",
        "    return covariance.mean(dim=1) # непонятно d это размерность вектора или на единицу меньше\n",
        "\n",
        "def combined_feature_extractor_loss(anchor, positive, negative, lambda1=1.0, lambda2=1.0, lambda3=1.0):\n",
        "    triplet_loss = adaptive_margin_triplet_loss(anchor, positive, negative)\n",
        "\n",
        "    var_loss = sum([variance_loss(embeddings) for embeddings in [anchor, positive, negative]])\n",
        "    cov_loss = sum([covariance_loss(embeddings) for embeddings in [anchor, positive, negative]])\n",
        "    return lambda1*triplet_loss + lambda2*var_loss + lambda3*cov_loss"
      ],
      "metadata": {
        "id": "hgmrs63BCbKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Miscellaneuos"
      ],
      "metadata": {
        "id": "90rkyIzDcLJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import mxnet as mx\n",
        "from mxnet import recordio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data_iter = mx.image.ImageIter(\n",
        "    batch_size=4,\n",
        "    data_shape=(3, 112, 112),\n",
        "    path_imgrec=\"./Data/casia-webface/train.rec\",\n",
        "    path_imgidx=\"./Data/casia-webface/train.idx\",\n",
        ")\n",
        "data_iter.reset()\n",
        "for j in range(4):\n",
        "    batch = data_iter.next()\n",
        "    data = batch.data[0]\n",
        "    # print(batch)\n",
        "    label = batch.label[0].asnumpy()\n",
        "    for i in range(4):\n",
        "        ax = plt.subplot(1, 4, i + 1)\n",
        "        plt.imshow(data[i].asnumpy().astype(np.uint8).transpose((1, 2, 0)))\n",
        "        ax.set_title(\"class: \" + str(label[i]))\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# # ======= Code to show single image =======#\n",
        "# path_imgrec = \"./Data/casia-webface/train.rec\"\n",
        "# path_imgidx = \"./Data/casia-webface/train.idx\"\n",
        "# imgrec = recordio.MXIndexedRecordIO(path_imgidx, path_imgrec, \"r\")\n",
        "# # %% 1 ~ 409623\n",
        "# # for i in range(409623):\n",
        "# for i in range(10):\n",
        "#     header, s = recordio.unpack(imgrec.read_idx(i + 1))\n",
        "#     img = mx.image.imdecode(s).asnumpy()\n",
        "#     plt.imshow(img)\n",
        "#     plt.title(\"id=\" + str(i) + \"label=\" + str(header.label))\n",
        "#     plt.pause(0.1)\n"
      ],
      "metadata": {
        "id": "dcMSCYbtVoqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ROOT = \"./Data/casia-webface/\"\n",
        "INPUT_SIZE=[112, 112]\n",
        "with open(os.path.join(DATA_ROOT, \"property\"), \"r\") as f:\n",
        "        NUM_CLASS, h, w = [int(i) for i in f.read().split(\",\")]\n",
        "assert h == INPUT_SIZE[0] and w == INPUT_SIZE[1]\n",
        "print(\"Number of Training Classes: {}\".format(NUM_CLASS))\n"
      ],
      "metadata": {
        "id": "GbZjI-wyV7lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launch test"
      ],
      "metadata": {
        "id": "VZ97nkbfZl_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install facenet_pytorch"
      ],
      "metadata": {
        "id": "zS-p4GbcaI6f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from facenet_pytorch import MTCNN\n",
        "from PIL import Image, ImageDraw\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class VisualFeatureExtractor(nn.Module):\n",
        "    def __init__(self, feature_dim=300):\n",
        "        super(VisualFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.resnet = models.resnet18(weights='DEFAULT')\n",
        "        in_features = self.resnet.fc.in_features\n",
        "\n",
        "        self.resnet.fc = nn.Identity()\n",
        "\n",
        "        # linear projection\n",
        "        self.projector = nn.Linear(in_features, feature_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        x = self.projector(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FaceExtractor:\n",
        "    def __init__(self, feature_extractor, device='cpu'):\n",
        "        self.mtcnn = MTCNN()\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.device = device\n",
        "        ##notgud duplicate\n",
        "        self.img_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def extract_faces_embedding(self, frame):\n",
        "\n",
        "        ##notgud\n",
        "        if isinstance(frame, torch.Tensor):\n",
        "            frame = transforms.ToPILImage()(frame.squeeze(0))\n",
        "\n",
        "        # find faces\n",
        "        boxes, faces_confs = self.mtcnn.detect([frame])\n",
        "        boxes = boxes[0]\n",
        "        if faces_confs is not None and boxes is not None:\n",
        "            areas = [(box[2] - box[0]) * (box[3] - box[1]) for box in boxes]\n",
        "            total_area = sum(areas)\n",
        "            # weight embeddings by areas of faces\n",
        "            weights = [area / total_area for area in areas]\n",
        "\n",
        "            face_embedding = torch.zeros(self.feature_extractor.projector.out_features).to(self.device)\n",
        "\n",
        "            for i, box in enumerate(boxes):\n",
        "                face = frame.crop((box[0], box[1], box[2], box[3]))\n",
        "                face_tensor = self.img_transform(face).unsqueeze(0).to(self.device)\n",
        "                face_emb = self.feature_extractor(face_tensor).squeeze(0)\n",
        "\n",
        "                face_embedding += face_emb * weights[i]\n",
        "\n",
        "            return face_embedding\n",
        "        else:\n",
        "            # faces wasn't found\n",
        "            return torch.zeros(self.feature_extractor.projector.out_features).to(self.device)\n",
        "\n",
        "\n",
        "class VideoFeatureExtractor(nn.Module):\n",
        "    def __init__(self, feature_dim=300, device='cpu'):\n",
        "        super(VideoFeatureExtractor, self).__init__()\n",
        "        self.visual_feature_extractor = VisualFeatureExtractor(feature_dim)\n",
        "        self.face_extractor = FaceExtractor(self.visual_feature_extractor, device=device)\n",
        "        self.img_transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "            ])\n",
        "\n",
        "    def forward(self, frames):\n",
        "        scene_embeddings = []\n",
        "        face_embeddings = []\n",
        "\n",
        "        # scene, faces embeds for each frame\n",
        "        for frame in frames:\n",
        "\n",
        "            frame_emb = self.visual_feature_extractor(self.img_transform(frame).unsqueeze(0))\n",
        "            scene_embeddings.append(frame_emb)\n",
        "\n",
        "            face_emb = self.face_extractor.extract_faces_embedding(self.img_transform(frame).unsqueeze(0))\n",
        "            face_embeddings.append(face_emb)\n",
        "\n",
        "        # max pooling\n",
        "        scene_embeddings = torch.stack(scene_embeddings)\n",
        "        scene_embeddings = scene_embeddings.permute(1, 0, 2).squeeze(0)\n",
        "        scene_embeddings = torch.max(scene_embeddings, dim=0)[0]\n",
        "\n",
        "        face_embeddings = torch.stack(face_embeddings)\n",
        "        face_embeddings = torch.max(face_embeddings, dim=0)[0]\n",
        "\n",
        "        final_embedding = torch.cat((scene_embeddings, face_embeddings), dim=0)\n",
        "        return final_embedding\n",
        "\n",
        "\n",
        "def get_pil_frames(video_path, num_frames=15, device='cpu'):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "\n",
        "    while frame_count < num_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(Image.fromarray(frame))\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) < num_frames:\n",
        "        raise ValueError(f\"Video contains fewer than {num_frames} frames.\")\n",
        "\n",
        "    return frames\n",
        "\n",
        "\n",
        "def display_frame(frame):\n",
        "    plt.imshow(frame)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "feature_dim = 300\n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = 'cpu'\n",
        "video_feature_extractor = VideoFeatureExtractor(feature_dim=feature_dim, device=device).to(device)\n",
        "\n",
        "video_path = \"./dev_splits_complete/dia0_utt1.mp4\"\n",
        "frames = get_pil_frames(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    embedding = video_feature_extractor(frames)\n",
        "print(\"Embedding shape:\", embedding.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPjQ8JMqZXCb",
        "outputId": "96f244a1-90f5-42b9-f084-280d04d10e35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: torch.Size([600])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PJt7M1RGe3AX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}